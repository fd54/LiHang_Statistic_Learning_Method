{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树是一种基本的分类回归方法\n",
    "\n",
    "通常包括三个步骤：特征选择、决策树的生成和决策树的修剪\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 决策树模型与学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。\n",
    "\n",
    "决策树可以看作是if-then规则的集合\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。\n",
    "\n",
    "如果利用一个特征进行分离的结果与随即分类的结果没有很大差别，则称这个特征没有分类能力。\n",
    "\n",
    "通常特征选择的准则是信息增益或者信息增益率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "熵和条件熵\n",
    "\n",
    "熵（entropy）表示随机变量不确定性的度量，设X是一个有限个取值的离散随机变量，其概率分布为P（X=i）= pi,i = 1,2,...,n\n",
    "\n",
    "则随机变量X的熵定义为\n",
    "\n",
    "H（X）= -sigma(pi*log(pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设有随机变量（X，Y），其联合概率分布为P(X=i,Y=j)=pij\n",
    "\n",
    "条件熵H（Y|X）表示在已知随机变量X的条件下随机变量Y的不确定性。\n",
    "\n",
    "随机变量X给定的条件下随机变量Y的条件熵定义为\n",
    "\n",
    "H（Y|X）= sigma(pi*(H(Y|Xi)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益表示得知特征X的信息而使类Y的不确定性减少的程度\n",
    "\n",
    "定义 信息增益：特征A对训练数据集D的信息增益g(D,A),定义为集合D的经验熵H(D)和特征A给定条件下的经验条件熵H(D|A)之差。\n",
    "\n",
    "一般的，熵和条件熵之差称为‘互信息’\n",
    "\n",
    "根据信息增益准则的特征选取方法是选择信息增益最大的特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益比\n",
    "\n",
    "信息增益值的大小是相对与训练数据集而言的。\n",
    "\n",
    "在分类问题困难时，也就是训练数据集的经验熵大的时候，信息增益值会偏大\n",
    "\n",
    "使用信息增益比可以对这一问题进行校正。\n",
    "\n",
    "信息增益比定义为信息增益和经验熵之比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 决策树的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3算法\n",
    "\n",
    "核心是在决策树各个结点上应用信息增益准则选择特征，递归的构造决策树，直到所有特征的信息增益均很小或没有特征可以选取为止\n",
    "\n",
    "信息增益的阈值设为ε"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C4.5算法\n",
    "\n",
    "与ID3相似，并对其做了改进，使用信息增益率来选择特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 决策树的剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树算法递归的产生决策树，往往对训练数据有很好的分类能力，但对未知数据的分类就没有那么准确，即出现过拟合的现象。\n",
    "\n",
    "其原因在于决策树的深度过大，从而决策树过于复杂，解决办法是对决策树进行剪枝，降低决策树的复杂度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的剪枝往往通过极小化决策树整体的损失函数或代价函数来实现。\n",
    "\n",
    "加上与叶结点个数有关的罚函数即可,相当于进行正则化\n",
    "\n",
    "设树的叶结点个数为|T|，t是树T的叶结点\n",
    "\n",
    "C_alpha(T) = C(T)+alpha*|T|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "树的剪枝算法\n",
    "\n",
    "(1)计算每个结点的经验熵\n",
    "\n",
    "(2)递归的从树的叶结点向上回缩，比较前后整体树的损失函数，决定是否进行剪枝\n",
    "\n",
    "(3)返回（2）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的剪枝算法可以由一种动态规划的算法实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 CART算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类与回归树，既可以用于分类，也可以用于回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归树\n",
    "\n",
    "划分输入空间，拟合区域输出的值，采用最小二乘法，生成决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类树\n",
    "\n",
    "采用基尼指数选取特征，Gini（P）= 1 - sigma(p^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier as DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([['青年', '否', '否', '一般', '否'],\n",
    "               ['青年', '否', '否', '好', '否'],\n",
    "               ['青年', '是', '否', '好', '是'],\n",
    "               ['青年', '是', '是', '一般', '是'],\n",
    "               ['青年', '否', '否', '一般', '否'],\n",
    "               ['中年', '否', '否', '一般', '否'],\n",
    "               ['中年', '否', '否', '好', '否'],\n",
    "               ['中年', '是', '是', '好', '是'],\n",
    "               ['中年', '否', '是', '非常好', '是'],\n",
    "               ['中年', '否', '是', '非常好', '是'],\n",
    "               ['老年', '否', '是', '非常好', '是'],\n",
    "               ['老年', '否', '是', '好', '是'],\n",
    "               ['老年', '是', '否', '好', '是'],\n",
    "               ['老年', '是', '否', '非常好', '是'],\n",
    "               ['老年', '否', '否', '一般', '否'] ])\n",
    "X[X=='青年'] = 1\n",
    "X[X=='中年'] = 2\n",
    "X[X=='老年'] = 3\n",
    "X[X=='是'] = 1\n",
    "X[X=='否'] = 0\n",
    "X[X=='一般'] = 1\n",
    "X[X=='好'] = 2\n",
    "X[X=='非常好'] = 3\n",
    "X_train = X[:,:4]  # 训练集特征\n",
    "y_train = X[:,-1]   # 训练集标签\n",
    "X_test = np.array([['青年','否','一般','是'],\n",
    "                ['中年','是','好','否'],\n",
    "                ['老年','否','一般','是']])\n",
    "X_test[X_test=='青年'] = 1\n",
    "X_test[X_test=='中年'] = 2\n",
    "X_test[X_test=='老年'] = 3\n",
    "X_test[X_test=='是'] = 1\n",
    "X_test[X_test=='否'] = 0\n",
    "X_test[X_test=='一般'] = 1\n",
    "X_test[X_test=='好'] = 2\n",
    "X_test[X_test=='非常好'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([['1', '0', '0', '1', '0'],\n",
       "        ['1', '0', '0', '2', '0'],\n",
       "        ['1', '1', '0', '2', '1'],\n",
       "        ['1', '1', '1', '1', '1'],\n",
       "        ['1', '0', '0', '1', '0'],\n",
       "        ['2', '0', '0', '1', '0'],\n",
       "        ['2', '0', '0', '2', '0'],\n",
       "        ['2', '1', '1', '2', '1'],\n",
       "        ['2', '0', '1', '3', '1'],\n",
       "        ['2', '0', '1', '3', '1'],\n",
       "        ['3', '0', '1', '3', '1'],\n",
       "        ['3', '0', '1', '2', '1'],\n",
       "        ['3', '1', '0', '2', '1'],\n",
       "        ['3', '1', '0', '3', '1'],\n",
       "        ['3', '0', '0', '1', '0']], dtype='<U3'), array([['1', '0', '1', '1'],\n",
       "        ['2', '1', '2', '0'],\n",
       "        ['3', '0', '1', '1']], dtype='<U2'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '1', '1'], dtype='<U3')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DT()\n",
    "dt.fit(X_train,y_train)\n",
    "dt.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs231n]",
   "language": "python",
   "name": "conda-env-cs231n-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
